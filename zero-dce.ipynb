{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def configure_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "# Call the GPU configuration function before importing TensorFlow\n",
    "configure_gpu()\n",
    "\n",
    "\n",
    "# ... (Other imports and rest of the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd8cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "268d85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0705f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11a9e3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: <BatchDataset element_spec=TensorSpec(shape=(16, 512, 512, 3), dtype=tf.float32, name=None)>\n",
      "Validation Dataset: <BatchDataset element_spec=TensorSpec(shape=(16, 512, 512, 3), dtype=tf.float32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 16\n",
    "MAX_TRAIN_IMAGES = 400\n",
    "\n",
    "\n",
    "\n",
    "def load_data(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def data_generator(low_light_images):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_low_light_images = sorted(glob.glob(\"./LOLdataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n",
    "val_low_light_images = sorted(glob.glob(\"./LOLdataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n",
    "test_low_light_images = sorted(glob.glob(\"./LOLdataset/eval15/low/*\"))\n",
    "\n",
    "train_dataset = data_generator(train_low_light_images)\n",
    "val_dataset = data_generator(val_low_light_images)\n",
    "\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Validation Dataset:\", val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75fe2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dce_net():\n",
    "    input_img = keras.Input(shape=[512, 512, 3])\n",
    "    conv1 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(input_img)\n",
    "    conv2 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(conv1)\n",
    "    conv3 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(conv2)\n",
    "    conv4 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(conv3)\n",
    "    int_con1 = layers.Concatenate(axis=-1)([conv4, conv3])\n",
    "    conv5 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(int_con1)\n",
    "    int_con2 = layers.Concatenate(axis=-1)([conv5, conv2])\n",
    "    conv6 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(int_con2)\n",
    "    int_con3 = layers.Concatenate(axis=-1)([conv6, conv1])\n",
    "    x_r = layers.Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(\n",
    "        int_con3\n",
    "    )\n",
    "    return keras.Model(inputs=input_img, outputs=x_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "893143e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_constancy_loss(x):\n",
    "    mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n",
    "    mr, mg, mb = (\n",
    "        mean_rgb[:, :, :, 0],\n",
    "        mean_rgb[:, :, :, 1],\n",
    "        mean_rgb[:, :, :, 2],\n",
    "    )\n",
    "    d_rg = tf.square(mr - mg)\n",
    "    d_rb = tf.square(mr - mb)\n",
    "    d_gb = tf.square(mb - mg)\n",
    "    return tf.sqrt(tf.square(d_rg) + tf.square(d_rb) + tf.square(d_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a17ca5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_loss(x, mean_val=0.6):\n",
    "    x = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "    mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\")\n",
    "    return tf.reduce_mean(tf.square(mean - mean_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcc394f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def illumination_smoothness_loss(x):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    h_x = tf.shape(x)[1]\n",
    "    w_x = tf.shape(x)[2]\n",
    "    count_h = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\n",
    "    count_w = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\n",
    "    h_tv = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : h_x - 1, :, :])))\n",
    "    w_tv = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : w_x - 1, :])))\n",
    "    batch_size = tf.cast(batch_size, dtype=tf.float32)\n",
    "    count_h = tf.cast(count_h, dtype=tf.float32)\n",
    "    count_w = tf.cast(count_w, dtype=tf.float32)\n",
    "    return 2 * (h_tv / count_h + w_tv / count_w) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "950b0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialConsistencyLoss(keras.losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(reduction=\"none\")\n",
    "\n",
    "        self.left_kernel = tf.constant(\n",
    "            [[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "        self.right_kernel = tf.constant(\n",
    "            [[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "        self.up_kernel = tf.constant(\n",
    "            [[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "        self.down_kernel = tf.constant(\n",
    "            [[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        original_mean = tf.reduce_mean(y_true, 3, keepdims=True)\n",
    "        enhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)\n",
    "        original_pool = tf.nn.avg_pool2d(\n",
    "            original_mean, ksize=4, strides=4, padding=\"VALID\"\n",
    "        )\n",
    "        enhanced_pool = tf.nn.avg_pool2d(\n",
    "            enhanced_mean, ksize=4, strides=4, padding=\"VALID\"\n",
    "        )\n",
    "\n",
    "        d_original_left = tf.nn.conv2d(\n",
    "            original_pool,\n",
    "            self.left_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "        d_original_right = tf.nn.conv2d(\n",
    "            original_pool,\n",
    "            self.right_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "        d_original_up = tf.nn.conv2d(\n",
    "            original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "        d_original_down = tf.nn.conv2d(\n",
    "            original_pool,\n",
    "            self.down_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "\n",
    "        d_enhanced_left = tf.nn.conv2d(\n",
    "            enhanced_pool,\n",
    "            self.left_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "        d_enhanced_right = tf.nn.conv2d(\n",
    "            enhanced_pool,\n",
    "            self.right_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "        d_enhanced_up = tf.nn.conv2d(\n",
    "            enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "        d_enhanced_down = tf.nn.conv2d(\n",
    "            enhanced_pool,\n",
    "            self.down_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "\n",
    "        d_left = tf.square(d_original_left - d_enhanced_left)\n",
    "        d_right = tf.square(d_original_right - d_enhanced_right)\n",
    "        d_up = tf.square(d_original_up - d_enhanced_up)\n",
    "        d_down = tf.square(d_original_down - d_enhanced_down)\n",
    "        return d_left + d_right + d_up + d_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04a661b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroDCE(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dce_model = build_dce_net()\n",
    "        #self.optimizer = keras.optimizers.Adam(learning_rate=1e-4) \n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True)\n",
    "    \n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.illumination_smoothness_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"illumination_smoothness_loss\"\n",
    "        )\n",
    "        self.spatial_constancy_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"spatial_constancy_loss\"\n",
    "        )\n",
    "        self.color_constancy_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"color_constancy_loss\"\n",
    "        )\n",
    "        self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.illumination_smoothness_loss_tracker,\n",
    "            self.spatial_constancy_loss_tracker,\n",
    "            self.color_constancy_loss_tracker,\n",
    "            self.exposure_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def get_enhanced_image(self, data, output):\n",
    "        r1 = output[:, :, :, :3]\n",
    "        r2 = output[:, :, :, 3:6]\n",
    "        r3 = output[:, :, :, 6:9]\n",
    "        r4 = output[:, :, :, 9:12]\n",
    "        r5 = output[:, :, :, 12:15]\n",
    "        r6 = output[:, :, :, 15:18]\n",
    "        r7 = output[:, :, :, 18:21]\n",
    "        r8 = output[:, :, :, 21:24]\n",
    "        x = data + r1 * (tf.square(data) - data)\n",
    "        x = x + r2 * (tf.square(x) - x)\n",
    "        x = x + r3 * (tf.square(x) - x)\n",
    "        enhanced_image = x + r4 * (tf.square(x) - x)\n",
    "        x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)\n",
    "        x = x + r6 * (tf.square(x) - x)\n",
    "        x = x + r7 * (tf.square(x) - x)\n",
    "        enhanced_image = x + r8 * (tf.square(x) - x)\n",
    "        return enhanced_image\n",
    "\n",
    "    def call(self, data):\n",
    "        dce_net_output = self.dce_model(data)\n",
    "        return self.get_enhanced_image(data, dce_net_output)\n",
    "\n",
    "    def compute_losses(self, data, output):\n",
    "        enhanced_image = self.get_enhanced_image(data, output)\n",
    "        loss_illumination = 200 * illumination_smoothness_loss(output)\n",
    "        loss_spatial_constancy = tf.reduce_mean(\n",
    "            self.spatial_constancy_loss(enhanced_image, data)\n",
    "        )\n",
    "        loss_color_constancy = 5 * tf.reduce_mean(color_constancy_loss(enhanced_image))\n",
    "        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n",
    "        total_loss = (\n",
    "            loss_illumination\n",
    "            + loss_spatial_constancy\n",
    "            + loss_color_constancy\n",
    "            + loss_exposure\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"illumination_smoothness_loss\": loss_illumination,\n",
    "            \"spatial_constancy_loss\": loss_spatial_constancy,\n",
    "            \"color_constancy_loss\": loss_color_constancy,\n",
    "            \"exposure_loss\": loss_exposure,\n",
    "        }\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self.dce_model(data)\n",
    "            losses = self.compute_losses(data, output)\n",
    "\n",
    "        '''gradients = tape.gradient(\n",
    "            losses[\"total_loss\"], self.dce_model.trainable_weights\n",
    "        )\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))'''\n",
    "        \n",
    "        gradients = tape.gradient(losses[\"total_loss\"], self.dce_model.trainable_weights)\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]  # Clip gradients\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n",
    "        self.illumination_smoothness_loss_tracker.update_state(\n",
    "            losses[\"illumination_smoothness_loss\"]\n",
    "        )\n",
    "        self.spatial_constancy_loss_tracker.update_state(\n",
    "            losses[\"spatial_constancy_loss\"]\n",
    "        )\n",
    "        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n",
    "        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n",
    "\n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        output = self.dce_model(data)\n",
    "        losses = self.compute_losses(data, output)\n",
    "\n",
    "        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n",
    "        self.illumination_smoothness_loss_tracker.update_state(\n",
    "            losses[\"illumination_smoothness_loss\"]\n",
    "        )\n",
    "        self.spatial_constancy_loss_tracker.update_state(\n",
    "            losses[\"spatial_constancy_loss\"]\n",
    "        )\n",
    "        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n",
    "        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n",
    "\n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
    "        \"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\n",
    "        self.dce_model.save_weights(\n",
    "            filepath,\n",
    "            overwrite=overwrite,\n",
    "            save_format=save_format,\n",
    "            options=options,\n",
    "        )\n",
    "\n",
    "    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n",
    "        \"\"\"While loading the weights, we simply load the weights of the DCE-Net\"\"\"\n",
    "        self.dce_model.load_weights(\n",
    "            filepath=filepath,\n",
    "            by_name=by_name,\n",
    "            skip_mismatch=skip_mismatch,\n",
    "            options=options,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "523535ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a25345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=f\"./modelweights/weights_zerodce_{current_time}.h5\",\n",
    "    monitor=\"val_total_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"min\",\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Create an EarlyStopping callback to stop training if there's no improvement\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_total_loss\",\n",
    "    patience=10,\n",
    "    mode=\"min\",\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "zero_dce_model = ZeroDCE()\n",
    "zero_dce_model.compile()\n",
    "history = zero_dce_model.fit(train_dataset, validation_data=val_dataset, epochs=100,callbacks=[checkpoint_callback, early_stopping_callback])\n",
    "#weights_file_path = \"./modelweights/weights_zerodce.h5\"\n",
    "\n",
    "\n",
    "\n",
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"total_loss\")\n",
    "plot_result(\"illumination_smoothness_loss\")\n",
    "plot_result(\"spatial_constancy_loss\")\n",
    "plot_result(\"color_constancy_loss\")\n",
    "plot_result(\"exposure_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b590362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_dce_model = ZeroDCE()\n",
    "zero_dce_model.load_weights(\"./modelweights/weights_zerodce_20240504_051644(512x512).h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c69c59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame extraction completed!\n",
      "Video created successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to the input video file\n",
    "video_path = './Jump_8_15.mp4'\n",
    "\n",
    "# Open the video file\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the total number of frames in the video\n",
    "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Initialize the frame counter\n",
    "frame_count = 0\n",
    "\n",
    "# Initialize a list to store the frames\n",
    "frames = []\n",
    "\n",
    "# Iterate over the frames of the video\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    # If no more frames, break the loop\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Append the frame to the list\n",
    "    frames.append(frame)\n",
    "\n",
    "    # Increment the frame counter\n",
    "    frame_count += 1\n",
    "\n",
    "    # Print the progress\n",
    "    \n",
    "\n",
    "# Release the video object\n",
    "video.release()\n",
    "\n",
    "print(\"Frame extraction completed!\")\n",
    "\n",
    "\n",
    "\n",
    "def infer(frames):\n",
    "    enhanced_frames = []\n",
    "\n",
    "    for frame in frames:\n",
    "        image = Image.fromarray(frame)\n",
    "        original_size = image.size\n",
    "        image = image.resize((512, 512))  # Resize the image to (192, 192)\n",
    "        image = img_to_array(image)\n",
    "        image = image.astype(\"float32\") / 255.0\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        output_image = zero_dce_model(image)\n",
    "        output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n",
    "        output_image = Image.fromarray(output_image.numpy())\n",
    "        output_image = output_image.resize(original_size)  # Resize the enhanced image back to the original size\n",
    "\n",
    "        enhanced_frame = np.array(output_image)\n",
    "        enhanced_frames.append(enhanced_frame)\n",
    "\n",
    "\n",
    "    return enhanced_frames\n",
    "\n",
    "# Assuming you have the 'frames' list containing the extracted frames\n",
    "\n",
    "enhanced_frames = infer(frames)\n",
    "\n",
    "    \n",
    "\n",
    "# Output video file name\n",
    "output_video = './resultvideo.mp4'\n",
    "\n",
    "# Frame rate of the video (frames per second)\n",
    "fps = 30\n",
    "\n",
    "# Get the frame size from the first frame\n",
    "height, width, _ = enhanced_frames[0].shape\n",
    "\n",
    "# Create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "# Iterate over the enhanced frames and write them to the video\n",
    "for frame in enhanced_frames:\n",
    "    video_writer.write(frame)\n",
    "\n",
    "# Release the VideoWriter object\n",
    "video_writer.release()\n",
    "\n",
    "print(\"Video created successfully!\")\n",
    "\n",
    "# Display the video\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d775f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflowenv",
   "language": "python",
   "name": "tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
